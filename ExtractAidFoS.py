#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar 25 15:57:24 2022

@author: aixuexi
"""
import json
import pickle
import os
import math
from tqdm import tqdm

# 数据预处理模块
# 抽取对象: 超过2 * m篇文章的作者 (改成所有作者, 即m=0 2022-3-31)
# 抽取内容: 抽取以上作者的发文量情况 和 引用情况
#           抽取以上每篇文章的fos列表
# 重新编号FoS压缩空间

#%%
# 数据挂载盘
abs_path = "/mnt/disk2/"
# MAG的数据路径
mag_papers_meta_cs = abs_path + "MAG_DATA_SET/MAGv2.1-meta-cs/mag_papers_{}.txt"
# 预处理数据存放路径
process_data_path = abs_path + "EmpiricalData/StatisticalData"
# 
m = 0
field_name = "computer science"


def Step1_ExtractAid2Nop():
    # Step 1.1 : 统计所有作者的发文量; Key: Aid Value: 作者发文量
    aid2nop = dict()
    for i in tqdm(range(0, 17)):
        with open(mag_papers_meta_cs.format(i), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    oneline_json = json.loads(oneline)
                    fos_list = oneline_json['f']
                    year = oneline_json['t']
                    pid = oneline_json['pid']
                    aid_list = oneline_json['aid']
                    aid_list = [aid for aid, org_id in aid_list]
                    for aid in aid_list:
                        if aid not in aid2nop:
                            aid2nop[aid] = 1
                        else:
                            aid2nop[aid] += 1
                else:
                    break    
    # Step 1.2: 根据发文量剔除发文量过少的作者
    total_num = len(aid2nop)
    count_num = 0
    aid2nop_part = dict()
    for aid in aid2nop:
        nop = aid2nop[aid]
        if nop < m * 2:
            count_num += 1
        else:
            aid2nop_part[aid] = nop
    print("发文量 <{}  涵盖{:.4f}authors".format(m * 2, count_num / total_num))
    print("发文量 >={} 涵盖{}authors".format(m * 2, total_num - count_num))
    
    # 储存
    # with open(os.path.join(process_data_path, "aid2nop.pkl"), 'wb') as f:
    #     pickle.dump(aid2nop, f)
    # 发文量大于2m的作者是我们的实验对象
    with open(os.path.join(process_data_path, "aid2nop.pkl"), 'wb') as f:
        pickle.dump(aid2nop_part, f)


def Step2_ExtractAidMeta():
    # 读取CS领域发文量超过2m的作者
    with open(os.path.join(process_data_path, "aid2nop.pkl"), 'rb') as f:
        aid2nop_part = pickle.load(f)
    
    # Step 2.1: 抽取这些作者(aid2nop_part)的文章 meta 信息 (fos, pubyear, aids)
    f_meta = open(os.path.join(process_data_path, "aid_meta.txt"), 'w')
    for i in tqdm(range(0, 17)):
        with open(mag_papers_meta_cs.format(i), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    oneline_json = json.loads(oneline)
                    fos_list = oneline_json['f']
                    year = oneline_json['t']
                    pid = oneline_json['pid']
                    aid_list = oneline_json['aid']
                    aid_list = [aid for aid, org_id in aid_list]
                    for aid in aid_list:  # 这篇文章包含需要的作者
                        if aid in aid2nop_part:
                            f_meta.write(oneline + "\n")  
                            break
                else:
                    break
    f_meta.close()
    
    # 引用关系统计
    # Step 2.2 : 抽取aid2nop_part所涉及的所有文章id
    aid_part_pid = dict()  # aid_part_meta 这些作者的文章
    with open(os.path.join(process_data_path, "aid_meta.txt"), 'r') as f:
        while True:
            oneline = f.readline().strip()
            if oneline:
                oneline_json = json.loads(oneline)
                fos_list = oneline_json['f']
                year = oneline_json['t']
                pid = oneline_json['pid']
                aid_list = oneline_json['aid']
                aid_list = [aid for aid, org_id in aid_list]
                # 我们需要获取这些pid的 citation count
                aid_part_pid[pid] = 0
            else:
                break
    
    # Step 2.3: 抽取抽取aid2nop_part所涉及的所有文章的被引用数目
    f_citations = open(os.path.join(process_data_path, "pid_citations.txt"), 'w')
    for i in tqdm(range(0, 17)):
        with open(mag_papers_meta_cs.format(i), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    oneline_json = json.loads(oneline)
                    # fos_list = oneline_json['f']
                    year = oneline_json['t']
                    pid = oneline_json['pid']
                    # aid_list = oneline_json['aid']
                    # aid_list = [aid for aid, org_id in aid_list]
                    ref = oneline_json['r']
                    for ref_i in ref:
                        if ref_i in aid_part_pid:
                            # ref_i 在year年 被pid引用 (只统计被引用, 不统计被那篇文章引用)
                            # 格式: "ref_i_id ; year"
                            f_citations.write("{}".format(ref_i) + ";" + "{}".format(year) + "\n")
                else:
                    break
    f_citations.close()


def Step3_CreateFoS2Idx():
    # 统计每个fos的频率, 最早出生年份
    fos2nop = dict()  
    for i in tqdm(range(0, 17)):
        with open(mag_papers_meta_cs.format(i), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    oneline_json = json.loads(oneline)
                    fos_list = oneline_json['f']
                    year = oneline_json['t']
                    if year == '':
                        continue
                    for fos in fos_list:
                        if fos not in fos2nop:
                            fos2nop[fos] = (1, year)
                        else:
                            nop, temp_year = fos2nop[fos]
                            nop += 1
                            temp_year = min(year, temp_year)
                            fos2nop[fos] = (nop, temp_year)
                else:
                    break
    del fos2nop[field_name]
    
    # 根据频率进行排序降序排序
    sortedfos = [(fos, fos2nop[fos][0]) for fos in fos2nop]
    sortedfos = sorted(sortedfos, key=lambda x: x[-1], reverse=True) 
    # 引入编号
    fos2idx = dict()
    idx2fos = dict()
    for idx, (fos, nop) in enumerate(sortedfos):
        fos2idx[fos] = idx
        idx2fos[idx] = fos
        
    with open(os.path.join(process_data_path, "fos2idx.pkl"), 'wb') as f:
        pickle.dump(fos2idx, f)
    with open(os.path.join(process_data_path, "idx2fos.pkl"), 'wb') as f:
        pickle.dump(idx2fos, f)

               
def Step4_ExtractAidFoS():
    
    # 读入 FoS编号
    with open(os.path.join(process_data_path, "fos2idx.pkl"), 'rb') as f:
        fos2idx = pickle.load(f)
    
    # 将以上发文量大于2m的作者的, 分发文量数目存放
    threshold = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 1e5]
    # 分块存放作者的fos信息
    for i, _ in enumerate(threshold):
        if i + 1 == len(threshold):
            break
        # 读取CS领域发文量超过2m的作者
        with open(os.path.join(process_data_path, "aid2nop.pkl"), 'rb') as f:
            aid2nop_part = pickle.load(f)
            
        # 抽取发文量阈值内的作者
        down = threshold[i]
        up = threshold[i + 1]
        aid2fos = dict()
        for aid in aid2nop_part:
            nop = aid2nop_part[aid]
            if down <= nop and nop < up:
                aid2fos[aid] = dict()
        del aid2nop_part
        print("发文量阈值是({}-{}), 累计包括{}名作者".format(down, up, len(aid2fos)))
        
        # 抽取上述作者的文章的meta信息: pubyear, fos_list, aid_list
        with open(os.path.join(process_data_path, "aid_meta.txt"), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    oneline_json = json.loads(oneline)
                    fos_list = oneline_json['f']
                    idx_list = list()
                    for fos in fos_list:
                        if fos != field_name:
                            idx_list.append(fos2idx[fos])  # FoS的编号, 压缩空间
                    year = oneline_json['t']
                    pid = oneline_json['pid']
                    aid_list = oneline_json['aid']
                    aid_list = [aid for aid, org_id in aid_list]
                    for aid in aid_list:
                        if aid in aid2fos:
                            if year not in aid2fos[aid]:
                                aid2fos[aid][year] = list()
                            aid2fos[aid][year].append((pid, idx_list))
                else:
                    break
        
        with open(os.path.join(process_data_path, "aid2fos_{}.pkl".format(i)), 'wb') as f:
            pickle.dump(aid2fos, f)
    del fos2idx
    
    # 利用 Step2_ExtractAidMeta 中的citations记录和以上分块pid 抽取引用数目
    for i in range(len(threshold)):
        if i + 1 == len(threshold):
            break
        with open(os.path.join(process_data_path, "aid2fos_{}.pkl".format(i)), 'rb') as f:
            aid2fos = pickle.load(f)
        # 第 i 块含有如下pid_citations篇文章
        pid_citations = dict()
        for aid in aid2fos:
            for year in aid2fos[aid]:
                for pid, tt in aid2fos[aid][year]:
                    pid_citations[pid] = dict()
        # 统计 pid_citations 的 citations
        with open(os.path.join(process_data_path, "pid_citations.txt"), 'r') as f:
            while True:
                oneline = f.readline().strip()
                if oneline:
                    pid, year = oneline.split(";")
                    pid = int(pid)
                    year = int(year)
                    if pid in pid_citations:
                        if year not in pid_citations[pid]:
                            pid_citations[pid][year] = 1  # 只有被引用数目, 没有引用网络
                        else:
                            pid_citations[pid][year] += 1
                else:
                    break
                
        with open(os.path.join(process_data_path, "aid2cc_{}.pkl".format(i)), 'wb') as f:
            pickle.dump(pid_citations, f)
          
            
def Decompose_file():
    # 删除临时文件
    os.remove(os.path.join(process_data_path, "aid_meta.txt"))
    os.remove(os.path.join(process_data_path, "pid_citations.txt"))
    
    # aid_part2fos_0.pkl 过大, 拆分成3份
    i = 0
    with open(os.path.join(process_data_path, "aid2fos_{}.pkl".format(i)), 'rb') as f:
        aid2fos = pickle.load(f)
    
    Keys = list(aid2fos.keys())
    chunks = 4
    batchsize = math.ceil(len(Keys) / chunks)
    
    start = 0
    end = 0
    for j in range(chunks):
        end = min(start + batchsize, len(Keys))
        print(start, end)
        aid2fos_chunki = dict()
        for fos in Keys[start: end]:
            aid2fos_chunki[fos] = aid2fos[fos]    
            
        with open(os.path.join(process_data_path, "aid2fos_{}.{}.pkl".format(i, j)), 'wb') as f:
            pickle.dump(aid2fos_chunki, f)
        start = end
    
    
def Main():
    # 发文量超过2m的作者aid2nop_part
    Step1_ExtractAid2Nop()
    # 上述作者文章的meta信息 (pid: pubyear, fos_list, aid_list), 含被引用次数 (citation count)
    Step2_ExtractAidMeta()
    # 根据频率给fos编号
    Step3_CreateFoS2Idx()
    # 提取每个作者每篇文章的fos, fos2idx (编号)
    Step4_ExtractAidFoS()
    #
    Decompose_file()